{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtuUbHj0jt71"
   },
   "source": [
    "# Imports and Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmrKKlKZkbnY"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CsyTdBuTimEZ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8EaplNdlrzI"
   },
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QWUI-ueYluAj"
   },
   "outputs": [],
   "source": [
    "# DATA\n",
    "BATCH_SIZE = 256\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (32, 32, 3)\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 20\n",
    "\n",
    "# AUGMENTATION\n",
    "IMAGE_SIZE = 48  # We will resize input images to this size.\n",
    "PATCH_SIZE = 6  # Size of the patches to be extracted from the input images.\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
    "\n",
    "# ViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 128\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 4\n",
    "MLP_UNITS = [\n",
    "    PROJECTION_DIM * 2,\n",
    "    PROJECTION_DIM,\n",
    "]\n",
    "\n",
    "# TOKENLEARNER\n",
    "NUM_TOKENS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "o6iaIEIJkqP9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 40000\n",
      "Validation samples: 10000\n",
      "Testing samples: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 10:40:03.308778: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Get the CIFAR-10 dataset.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "(x_train, y_train), (x_val, y_val) = (\n",
    "    (x_train[:40000], y_train[:40000]),\n",
    "    (x_train[40000:], y_train[40000:]),\n",
    ")\n",
    "print(f\"Training samples: {len(x_train)}\")\n",
    "print(f\"Validation samples: {len(x_val)}\")\n",
    "print(f\"Testing samples: {len(x_test)}\")\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds = train_ds.shuffle(BATCH_SIZE * 100).batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_ds = val_ds.batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gZwShY42gM2"
   },
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uGTtuVR9nE3g"
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Rescaling(1 / 255.0),\n",
    "        layers.Resizing(INPUT_SHAPE[0] + 20, INPUT_SHAPE[0] + 20),\n",
    "        layers.RandomCrop(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ob3yvVA3E9u"
   },
   "source": [
    "# Patch Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_XZH_aS4nqEE"
   },
   "outputs": [],
   "source": [
    "def position_embedding(\n",
    "    projected_patches, num_patches=NUM_PATCHES, projection_dim=PROJECTION_DIM\n",
    "):\n",
    "    # Build the positions.\n",
    "    positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "\n",
    "    # Encode the positions with an Embedding layer.\n",
    "    encoded_positions = layers.Embedding(\n",
    "        input_dim=num_patches, output_dim=projection_dim\n",
    "    )(positions)\n",
    "\n",
    "    # Add encoded positions to the projected patches.\n",
    "    return projected_patches + encoded_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuUfoaGpoUxN"
   },
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mgr25Z2qoQb7"
   },
   "outputs": [],
   "source": [
    "def mlp(x, dropout_rate, hidden_units):\n",
    "    # Iterate over the hidden units and\n",
    "    # add Dense => Dropout.\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfXKWaNCogWJ"
   },
   "source": [
    "# TokenLearner\n",
    "\n",
    "From here on these symbols mean:\n",
    "- B: Batch Size\n",
    "- H: Height of the input\n",
    "- W: Width of the input\n",
    "- C: Channel size of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Oz10FLn6AsGg"
   },
   "outputs": [],
   "source": [
    "def token_learner(inputs, number_of_tokens=NUM_TOKENS):\n",
    "    # Layer normalize the inputs.\n",
    "    x = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(inputs)  # (B, H, W, C)\n",
    "\n",
    "    # Applying Conv2D => Reshape => Permute\n",
    "    # The reshape and permute is done to help with the next steps of\n",
    "    # multiplication and Global Average Pooling.\n",
    "    attention_maps = keras.Sequential(\n",
    "        [\n",
    "            # 3 layers of conv with gelu activation as suggested\n",
    "            # in the paper.\n",
    "            layers.Conv2D(\n",
    "                filters=number_of_tokens,\n",
    "                kernel_size=(3, 3),\n",
    "                activation=tf.nn.gelu,\n",
    "                padding=\"same\",\n",
    "                use_bias=False,\n",
    "            ),\n",
    "            layers.Conv2D(\n",
    "                filters=number_of_tokens,\n",
    "                kernel_size=(3, 3),\n",
    "                activation=tf.nn.gelu,\n",
    "                padding=\"same\",\n",
    "                use_bias=False,\n",
    "            ),\n",
    "            layers.Conv2D(\n",
    "                filters=number_of_tokens,\n",
    "                kernel_size=(3, 3),\n",
    "                activation=tf.nn.gelu,\n",
    "                padding=\"same\",\n",
    "                use_bias=False,\n",
    "            ),\n",
    "            # This conv layer will generate the attention maps\n",
    "            layers.Conv2D(\n",
    "                filters=number_of_tokens,\n",
    "                kernel_size=(3, 3),\n",
    "                activation=\"sigmoid\",  # Note sigmoid for [0, 1] output\n",
    "                padding=\"same\",\n",
    "                use_bias=False,\n",
    "            ),\n",
    "            # Reshape and Permute\n",
    "            layers.Reshape((-1, number_of_tokens)),  # (B, H*W, num_of_tokens)\n",
    "            layers.Permute((2, 1)),\n",
    "        ]\n",
    "    )(\n",
    "        x\n",
    "    )  # (B, num_of_tokens, H*W)\n",
    "\n",
    "    # Reshape the input to align it with the output of the conv block.\n",
    "    num_filters = inputs.shape[-1]\n",
    "    inputs = layers.Reshape((1, -1, num_filters))(inputs)  # inputs == (B, 1, H*W, C)\n",
    "\n",
    "    # Element-Wise multiplication of the attention maps and the inputs\n",
    "    attended_inputs = (\n",
    "        attention_maps[..., tf.newaxis] * inputs\n",
    "    )  # (B, num_tokens, H*W, C)\n",
    "\n",
    "    # Global average pooling the element wise multiplication result.\n",
    "    outputs = tf.reduce_mean(attended_inputs, axis=2)  # (B, num_tokens, C)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(encoded_patches):\n",
    "    # Layer normalization 1.\n",
    "    x1 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(encoded_patches)\n",
    "\n",
    "    # Multi Head Self Attention layer 1.\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n",
    "    )(x1, x1)\n",
    "\n",
    "    # Skip connection 1.\n",
    "    x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "    # Layer normalization 2.\n",
    "    x3 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(x2)\n",
    "\n",
    "    # MLP layer 1.\n",
    "    x4 = mlp(x3, hidden_units=MLP_UNITS, dropout_rate=0.1)\n",
    "\n",
    "    # Skip connection 2.\n",
    "    encoded_patches = layers.Add()([x4, x2])\n",
    "    return encoded_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3FSfbtFehYJ"
   },
   "source": [
    "## ViT model with optional TokenLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FKDtmPH-aNal"
   },
   "outputs": [],
   "source": [
    "def create_vit_classifier(use_token_learner=True, token_learner_units=NUM_TOKENS):\n",
    "    inputs = layers.Input(shape=INPUT_SHAPE)  # (B, H, W, C)\n",
    "\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "\n",
    "    # Create patches and project the pathces.\n",
    "    projected_patches = layers.Conv2D(\n",
    "        filters=PROJECTION_DIM,\n",
    "        kernel_size=(PATCH_SIZE, PATCH_SIZE),\n",
    "        strides=(PATCH_SIZE, PATCH_SIZE),\n",
    "        padding=\"VALID\",\n",
    "    )(\n",
    "        augmented\n",
    "    )\n",
    "    _, h, w, c = projected_patches.shape\n",
    "    projected_patches = layers.Reshape((h * w, c))(projected_patches)  # (B, number_patches, projection_dim)\n",
    "\n",
    "    # Add positional embeddings to the projected patches.\n",
    "    encoded_patches = position_embedding(projected_patches)  # (B, number_patches, projection_dim)\n",
    "    encoded_patches = layers.Dropout(0.1)(encoded_patches)\n",
    "\n",
    "    # Iterate over the number of layers and stack up blocks of\n",
    "    # Transformer.\n",
    "    for i in range(NUM_LAYERS):\n",
    "        # Add a Transformer block.\n",
    "        encoded_patches = transformer(encoded_patches)\n",
    "\n",
    "        # Add TokenLearner layer in the middle of the \n",
    "        # architecture. The paper suggests that anywhere\n",
    "        # between 1/2 or 3/4 will work well.\n",
    "        if use_token_learner and i == NUM_LAYERS // 2:\n",
    "            _, hh, c = encoded_patches.shape\n",
    "            h = int(math.sqrt(hh))\n",
    "            encoded_patches = layers.Reshape((h, h, c))(encoded_patches)  # (B, h, h, projection_dim)\n",
    "            encoded_patches = token_learner(\n",
    "                encoded_patches, token_learner_units\n",
    "            )  # (B, num_tokens, c)\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(encoded_patches)\n",
    "    representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(representation)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60oHNkcoef_5"
   },
   "source": [
    "## Training utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "sOmyzOhVfWoO"
   },
   "outputs": [],
   "source": [
    "def run_experiment(model, use_token_learner=True):\n",
    "    # Initialize the AdamW optimizer.\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    # Compile the model with the optimizer, loss function\n",
    "    # and the metrics.\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    # Define callbacks\n",
    "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_dir = \"logs-tokenlearner\" if use_token_learner else \"logs-no-tokenlearner\"\n",
    "    tensorborad_callback = keras.callbacks.TensorBoard(\n",
    "        log_dir=f\"{root_dir}-{timestamp}\"\n",
    "    )\n",
    "    \n",
    "    # Train the model.\n",
    "    _ = model.fit(\n",
    "        train_ds,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=[checkpoint_callback, tensorborad_callback],\n",
    "    )\n",
    "    \n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(test_ds)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "data_augmentation (Sequential)  (None, 48, 48, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 8, 8, 128)    13952       data_augmentation[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 64, 128)      0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 64, 128)      0           reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 64, 128)      0           tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 64, 128)      256         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention (MultiHead (None, 64, 128)      263808      layer_normalization[0][0]        \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 64, 128)      0           multi_head_attention[0][0]       \n",
      "                                                                 dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 64, 128)      256         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64, 256)      33024       layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64, 256)      0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64, 128)      32896       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64, 128)      0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 128)      0           dropout_2[0][0]                  \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 64, 128)      256         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_1 (MultiHe (None, 64, 128)      263808      layer_normalization_2[0][0]      \n",
      "                                                                 layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 64, 128)      0           multi_head_attention_1[0][0]     \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 64, 128)      256         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64, 256)      33024       layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64, 256)      0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64, 128)      32896       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 64, 128)      0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 64, 128)      0           dropout_4[0][0]                  \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_4 (LayerNor (None, 64, 128)      256         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_2 (MultiHe (None, 64, 128)      263808      layer_normalization_4[0][0]      \n",
      "                                                                 layer_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 64, 128)      0           multi_head_attention_2[0][0]     \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_5 (LayerNor (None, 64, 128)      256         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64, 256)      33024       layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 64, 256)      0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64, 128)      32896       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 64, 128)      0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 64, 128)      0           dropout_6[0][0]                  \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 8, 8, 128)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_6 (LayerNor (None, 8, 8, 128)    256         reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 8, 64)        10944       layer_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (None, 8, 64, 1)     0           sequential[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1, 64, 128)   0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply (TFOpLambda)   (None, 8, 64, 128)   0           tf.__operators__.getitem[0][0]   \n",
      "                                                                 reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_mean (TFOpLambda (None, 8, 128)       0           tf.math.multiply[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_7 (LayerNor (None, 8, 128)       256         tf.math.reduce_mean[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_3 (MultiHe (None, 8, 128)       263808      layer_normalization_7[0][0]      \n",
      "                                                                 layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 8, 128)       0           multi_head_attention_3[0][0]     \n",
      "                                                                 tf.math.reduce_mean[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_8 (LayerNor (None, 8, 128)       256         add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 8, 256)       33024       layer_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 8, 256)       0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 8, 128)       32896       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 8, 128)       0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 128)       0           dropout_8[0][0]                  \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_9 (LayerNor (None, 8, 128)       256         add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 10)           1290        global_average_pooling1d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 1,347,658\n",
      "Trainable params: 1,347,658\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vit_token_learner = create_vit_classifier()\n",
    "vit_token_learner.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rm2Q2b6hgY-s"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBWjky5s4d-m"
   },
   "outputs": [],
   "source": [
    "# Should at least be 5.\n",
    "num_trials = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NMiWnSdgXXz"
   },
   "outputs": [],
   "source": [
    "# With TokenLearner.\n",
    "for _ in range(num_trials):\n",
    "    vit_token_learner = create_vit_classifier()\n",
    "    run_experiment(vit_token_learner, use_token_learner=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Im-aqSSzhaNr"
   },
   "outputs": [],
   "source": [
    "# Without TokenLearner.\n",
    "for _ in range(num_trials):\n",
    "    vit = create_vit_classifier(use_token_learner=False)\n",
    "    run_experiment(vit, use_token_learner=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKaOLovV4d-n"
   },
   "outputs": [],
   "source": [
    "create_vit_classifier().count_params(), create_vit_classifier(\n",
    "    use_token_learner=False\n",
    ").count_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWlhIayN4d-o"
   },
   "source": [
    "## References\n",
    "\n",
    "* [Official TokenLearner code](https://github.com/google-research/scenic/blob/main/scenic/projects/token_learner/model.py)\n",
    "* [Image Classification with ViTs on keras.io](https://keras.io/examples/vision/image_classification_with_vision_transformer/)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "TokenLearner.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m84"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
